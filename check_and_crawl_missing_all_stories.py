import os
import asyncio
import json
import datetime
import shutil
from typing import cast
from filelock import FileLock
from adapters.factory import get_adapter
from config.config import BASE_URLS, COMPLETED_FOLDER, DATA_FOLDER, LOADED_PROXIES, PROXIES_FILE, PROXIES_FOLDER
from config.proxy_provider import load_proxies
from scraper import initialize_scraper
from utils.chapter_utils import count_txt_files
from utils.logger import logger
from utils.async_utils import SEM
from utils.io_utils import create_proxy_template_if_not_exists, safe_write_file
from analyze.truyenfull_vision_parse import get_all_genres, get_all_stories_from_genre, get_chapters_from_story, get_story_details
from main import crawl_missing_chapters_for_story
from utils.notifier import send_telegram_notify
from utils.state_utils import get_missing_worker_state_file, load_crawl_state

MAX_CONCURRENT_STORIES = 3
STORY_SEM = asyncio.Semaphore(MAX_CONCURRENT_STORIES)


async def crawl_story_with_limit(
    site_key: str,
    session, 
    missing_chapters: list,
    metadata: dict,
    current_category: dict,
    story_folder: str,
    crawl_state: dict,
    num_batches: int = 10,
    state_file: str = None # type: ignore
):
    async with STORY_SEM:
        await crawl_missing_with_limit(
            site_key, session, missing_chapters, metadata,
            current_category, story_folder, crawl_state, num_batches,
            state_file=state_file
        )

def sync_metadata_total_chapters(story_folder):
    meta_path = os.path.join(story_folder, "metadata.json")
    if not os.path.exists(meta_path):
        return
    with open(meta_path, "r", encoding="utf-8") as f:
        meta = json.load(f)
    txt_files = [f for f in os.listdir(story_folder) if f.endswith('.txt')]
    if len(txt_files) > meta.get("total_chapters_on_site", 0):
        meta["total_chapters_on_site"] = len(txt_files)
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=4)
        print(f"[SYNC] ÄÃ£ cáº­p nháº­t láº¡i total_chapters_on_site cho '{os.path.basename(story_folder)}' thÃ nh {len(txt_files)}")


async def crawl_missing_with_limit(
    site_key: str,
    session,
    missing_chapters: list,
    metadata: dict,
    current_category: dict,
    story_folder: str,
    crawl_state: dict,
    num_batches: int = 10,
    state_file: str = None # type: ignore
):
    if not state_file:
        state_file = get_missing_worker_state_file(site_key)
    print(f"[START] Crawl missing for {metadata['title']} ...")
    async with SEM:
        result = await crawl_missing_chapters_for_story(
            site_key, session, missing_chapters, metadata,
            current_category, story_folder, crawl_state, num_batches,
            state_file=state_file
        )
    print(f"[DONE] Crawl missing for {metadata['title']} ...")
    return result

async def check_genre_complete_and_notify(genre_name, genre_url):
    stories_on_web = await get_all_stories_from_genre(genre_name, genre_url)
    completed_folder = os.path.join(COMPLETED_FOLDER, genre_name)
    completed_folders = os.listdir(completed_folder)
    completed_titles = []
    for folder in completed_folders:
        meta_path = os.path.join(completed_folder, folder, "metadata.json")
        if os.path.exists(meta_path):
            lock_path = meta_path + ".lock"
            lock = FileLock(lock_path, timeout=30)
            with lock:
                with open(meta_path, "r", encoding="utf-8") as f:
                    meta = json.load(f)
                    completed_titles.append(meta.get("title"))
    missing = [story for story in stories_on_web if story["title"] not in completed_titles]
    if not missing:
        await send_telegram_notify(f"ğŸ‰ ÄÃ£ crawl xong **Táº¤T Cáº¢** truyá»‡n cá»§a thá»ƒ loáº¡i [{genre_name}] trÃªn web!")

def get_auto_batch_count(fixed=None, default=10, min_batch=1, max_batch=20, num_items=None):
    if fixed is not None:
        return fixed
    batch = default
    if num_items:
        batch = min(batch, num_items)
    return min(batch, max_batch)

def parse_args():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--force-unskip', action='store_true', help='Bá» qua skip_crawl Ä‘á»ƒ crawl láº¡i toÃ n bá»™ truyá»‡n skip')
    return parser.parse_args()

def unskip_all_stories(data_folder):
    import os, json
    for folder in os.listdir(data_folder):
        story_folder = os.path.join(data_folder, folder)
        if not os.path.isdir(story_folder): continue
        meta_path = os.path.join(story_folder, "metadata.json")
        if not os.path.exists(meta_path): continue
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
        changed = False
        if meta.get("skip_crawl"):
            meta.pop("skip_crawl", None)
            changed = True
        if "meta_retry_count" in meta:
            meta.pop("meta_retry_count", None)
            changed = True
        if changed:
            with open(meta_path, "w", encoding="utf-8") as f:
                json.dump(meta, f, ensure_ascii=False, indent=4)
            print(f"Unskipped: {meta.get('title')}")


async def fix_metadata_with_retry(metadata, metadata_path, story_folder, site_key=None):
    """
    Retry tá»‘i Ä‘a 3 láº§n láº¥y láº¡i metadata náº¿u thiáº¿u total_chapters_on_site hoáº·c thiáº¿u url/title.
    Náº¿u fail, set skip_crawl vÃ  return False.
    """
    if metadata.get("skip_crawl", False):
        print(f"[SKIP] Truyá»‡n '{metadata.get('title')}' Ä‘Ã£ bá»‹ Ä‘Ã¡nh dáº¥u bá» qua (skip_crawl), khÃ´ng crawl láº¡i ná»¯a.")
        return False

    retry_count = metadata.get("meta_retry_count", 0)
    total_chapters = metadata.get("total_chapters_on_site")
    url = metadata.get("url")
    title = metadata.get("title")

    # Bá»• sung: láº¥y láº¡i url/title tá»« sources theo site_key
    for _ in range(3):
        if url and title:
            break
        # Æ¯u tiÃªn láº¥y url tá»« sources Ä‘Ãºng site_key
        if not url and metadata.get("sources"):
            url_found = None
            # Æ¯u tiÃªn Ä‘Ãºng site_key
            if site_key:
                for src in metadata["sources"]:
                    if src.get("site") == site_key and src.get("url"):
                        url_found = src["url"]
                        break
            # Náº¿u khÃ´ng tÃ¬m Ä‘Æ°á»£c, láº¥y url Ä‘áº§u tiÃªn cÃ³
            if not url_found:
                for src in metadata["sources"]:
                    if src.get("url"):
                        url_found = src["url"]
                        break
            if url_found:
                url = url_found
                print(f"[FIX] Bá»• sung láº¡i url cho '{story_folder}' theo site_key '{site_key}': {url}")
                metadata["url"] = url
        # Láº¥y láº¡i title tá»« sources (náº¿u cÃ³ title há»£p lá»‡), Æ°u tiÃªn Ä‘Ãºng site_key
        if not title and metadata.get("sources"):
            title_found = None
            if site_key:
                for src in metadata["sources"]:
                    if src.get("site") == site_key and src.get("title"):
                        title_found = src["title"]
                        break
            if not title_found:
                for src in metadata["sources"]:
                    if src.get("title"):
                        title_found = src["title"]
                        break
            if title_found:
                title = title_found
                print(f"[FIX] Bá»• sung láº¡i title cho '{story_folder}' tá»« sources: {title}")
                metadata["title"] = title
        # Náº¿u váº«n chÆ°a cÃ³, láº¥y láº¡i title tá»« folder
        if not title:
            folder_title = os.path.basename(story_folder).replace("-", " ").title()
            print(f"[FIX] Bá»• sung láº¡i title cho '{story_folder}' tá»« folder: {folder_title}")
            title = folder_title
            metadata["title"] = title
        retry_count += 1

    if not url or not title:
        print(f"[SKIP] '{story_folder}' thiáº¿u url/title (Ä‘Ã£ thá»­ 3 láº§n), khÃ´ng thá»ƒ láº¥y láº¡i metadata!")
        metadata["skip_crawl"] = True
        metadata["skip_reason"] = "missing_url_title"
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=4)
        return False

    # Náº¿u Ä‘Ã£ cÃ³ Ä‘á»§ url/title, tiáº¿p tá»¥c retry láº¥y metadata nhÆ° bÃ¬nh thÆ°á»ng
    retry_count = metadata.get("meta_retry_count", 0)
    while retry_count < 3 and (not total_chapters or total_chapters < 1):
        print(f"[SKIP] '{story_folder}' thiáº¿u total_chapters_on_site -> [FIXED] Äang láº¥y láº¡i metadata láº§n {retry_count+1} qua proxy...")
        details = await get_story_details(url, title)
        retry_count += 1
        metadata["meta_retry_count"] = retry_count
        if details and details.get("total_chapters_on_site"):
            print(f"[FIXED] Cáº­p nháº­t láº¡i metadata, tá»•ng chÆ°Æ¡ng: {details['total_chapters_on_site']}")
            metadata.update(details)
            metadata['total_chapters_on_site'] = details['total_chapters_on_site']
            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=4)
            return True
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=4)
        total_chapters = metadata.get("total_chapters_on_site")
    if not total_chapters or total_chapters < 1:
        print(f"[SKIP] '{story_folder}' láº¥y meta 3 láº§n váº«n lá»—i, sáº½ khÃ´ng crawl láº¡i truyá»‡n nÃ y ná»¯a!")
        metadata["skip_crawl"] = True
        metadata["skip_reason"] = "meta_failed"
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=4)
        return False
    return True



async def check_and_crawl_missing_all_stories(adapter, home_page_url, site_key, force_unskip=False):
    state_file = get_missing_worker_state_file(site_key)   # <--- dÃ¹ng file phá»¥!
    crawl_state = await load_crawl_state(state_file)
    all_genres = await get_all_genres(home_page_url)
    genre_name_to_url = {g['name']: g['url'] for g in all_genres}
    genre_complete_checked = set()
    os.makedirs(COMPLETED_FOLDER, exist_ok=True)
    await create_proxy_template_if_not_exists(PROXIES_FILE, PROXIES_FOLDER)
    await load_proxies(PROXIES_FILE)
    await initialize_scraper(site_key)


    # Láº¥y danh sÃ¡ch táº¥t cáº£ story folder cáº§n crawl
    story_folders = [
        os.path.join(DATA_FOLDER, cast(str, f))
        for f in os.listdir(DATA_FOLDER)
        if os.path.isdir(os.path.join(DATA_FOLDER, cast(str, f)))
    ]
    crawl_state = await load_crawl_state(state_file)
    tasks = []
    for story_folder in story_folders:
        if os.path.dirname(story_folder) == os.path.abspath(COMPLETED_FOLDER):
            continue
        metadata_path = os.path.join(story_folder, "metadata.json")
        if not os.path.exists(metadata_path):
            print(f"[SKIP] KhÃ´ng cÃ³ metadata.json trong {story_folder}")
            continue

        with open(metadata_path, "r", encoding="utf-8") as f:
            metadata = json.load(f)

                # Náº¿u force_unskip: XoÃ¡ skip_crawl & meta_retry_count náº¿u cÃ³
        if force_unskip:
            changed = False
            if metadata.get("skip_crawl"):
                metadata.pop("skip_crawl", None)
                changed = True
            if "meta_retry_count" in metadata:
                metadata.pop("meta_retry_count", None)
                changed = True
            if changed:
                with open(metadata_path, "w", encoding="utf-8") as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=4)
                print(f"[UNSKIP] Tá»± Ä‘á»™ng unskip: {metadata.get('title')}")
        # Skip náº¿u Ä‘Ã£ flag
        if metadata.get("skip_crawl", False):
            print(f"[SKIP] Truyá»‡n '{metadata.get('title')}' Ä‘Ã£ bá»‹ Ä‘Ã¡nh dáº¥u bá» qua (skip_crawl), khÃ´ng crawl láº¡i ná»¯a.")
            continue
        # Auto fix metadata náº¿u thiáº¿u (vÃ  skip náº¿u quÃ¡ 3 láº§n)
        if not await fix_metadata_with_retry(metadata, metadata_path, story_folder):
            continue

        total_chapters = metadata.get("total_chapters_on_site")
        genre_name = None
        if metadata.get('categories') and isinstance(metadata['categories'], list) and metadata['categories']:
            genre_name = metadata['categories'][0].get('name')
        if not genre_name:
            genre_name = "Unknown"

        crawled_files = count_txt_files(story_folder)
        if crawled_files < total_chapters:
            print(f"[MISSING] '{metadata['title']}' thiáº¿u chÆ°Æ¡ng ({crawled_files}/{total_chapters}) -> Äang kiá»ƒm tra/crawl bÃ¹ tá»« má»i nguá»“n...")

            # --- Bá»• sung crawl tá»« má»i nguá»“n ---
            for source in metadata.get("sources", []):
                url = source.get("url")
                if not site_key or not url:
                    continue
                adapter = get_adapter(site_key)
                try:
                    chapters = await adapter.get_chapter_list(url, metadata['title'])
                except Exception as ex:
                    print(f"  [ERROR] KhÃ´ng láº¥y Ä‘Æ°á»£c chapter list tá»« {site_key}: {ex}")
                    continue

                # Chá»‰ crawl nhá»¯ng chÆ°Æ¡ng thiáº¿u thá»±c sá»±
                existing_files = set(os.listdir(story_folder))
                missing_chapters = []
                for idx, ch in enumerate(chapters):
                    fname_only = f"{idx+1:04d}_{ch.get('title', 'untitled')}.txt"
                    if fname_only not in existing_files:
                        ch['idx'] = idx # type: ignore
                        missing_chapters.append(ch)
                if not missing_chapters:
                    print(f"  KhÃ´ng cÃ²n chÆ°Æ¡ng nÃ o thiáº¿u á»Ÿ nguá»“n {site_key}.")
                    continue

                print(f"  Báº¯t Ä‘áº§u crawl bá»• sung {len(missing_chapters)} chÆ°Æ¡ng tá»« nguá»“n {site_key}")
                current_category = metadata['categories'][0] if metadata.get('categories') and isinstance(metadata['categories'], list) and metadata['categories'] else {}
                num_batches = get_auto_batch_count(fixed=10)
                logger.info(f"Auto chá»n {num_batches} batch cho truyá»‡n {metadata['title']} (site: {site_key}, proxy usable: {len(LOADED_PROXIES)})")
                tasks.append(
                    crawl_story_with_limit(
                        site_key, None, missing_chapters, metadata, current_category,
                        story_folder, crawl_state, num_batches=num_batches, state_file=state_file
                    )
                )
    if tasks:
        await asyncio.gather(*tasks)

    # Move completed stories + notify
    for story_folder in story_folders:
        metadata_path = os.path.join(story_folder, "metadata.json")
        if not os.path.exists(metadata_path):
            continue

        with open(metadata_path, "r", encoding="utf-8") as f:
            metadata = json.load(f)
        if metadata.get("skip_crawl", False):
            print(f"[SKIP] Truyá»‡n '{metadata.get('title')}' Ä‘Ã£ bá»‹ Ä‘Ã¡nh dáº¥u bá» qua (skip_crawl), khÃ´ng crawl láº¡i ná»¯a.")
            continue
        # Láº§n ná»¯a, fix meta náº¿u thiáº¿u (trÃ¡nh lá»—i do cÃ³ thá»ƒ cÃ³ truyá»‡n chÆ°a retry)
        if not await fix_metadata_with_retry(metadata, metadata_path, story_folder):
            continue

        total_chapters = metadata.get("total_chapters_on_site")
        genre_name = None
        if metadata.get('categories') and isinstance(metadata['categories'], list) and metadata['categories']:
            genre_name = metadata['categories'][0].get('name')
        if not genre_name:
            genre_name = "Unknown"
        crawled_files = count_txt_files(story_folder)

        # ==== Kiá»ƒm tra fields báº¯t buá»™c ====
        fields_required = ['description', 'author', 'cover', 'categories', 'title', 'total_chapters_on_site']
        meta_ok = all(metadata.get(f) for f in fields_required)
        if not meta_ok:
            print(f"[SKIP] '{story_folder}' thiáº¿u trÆ°á»ng quan trá»ng, sáº½ cá»‘ gáº¯ng láº¥y láº¡i metadata...")
            details = await get_story_details(metadata.get("url"), metadata.get("title"))
            if details and all(details.get(f) for f in fields_required):
                print(f"[FIXED] ÄÃ£ bá»• sung metadata Ä‘á»§ cho '{metadata.get('title')}'")
                metadata.update(details)
                with open(metadata_path, "w", encoding="utf-8") as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=4)
            else:
                print(f"[ERROR] KhÃ´ng láº¥y Ä‘á»§ metadata cho '{metadata.get('title')}'! Sáº½ bá» qua move.")
                continue
        # ==== End check meta ====
        sync_metadata_total_chapters(story_folder)
        if crawled_files >= metadata.get("total_chapters_on_site"):
            dest_genre_folder = os.path.join(COMPLETED_FOLDER, genre_name)
            os.makedirs(dest_genre_folder, exist_ok=True)
            dest_folder = os.path.join(dest_genre_folder, os.path.basename(story_folder))
            if not os.path.exists(dest_folder):
                shutil.move(story_folder, dest_folder)
                print(f"[INFO] ÄÃ£ chuyá»ƒn truyá»‡n '{metadata['title']}' sang {dest_genre_folder}")
            if genre_name not in genre_complete_checked:
                genre_url = genre_name_to_url.get(genre_name)
                if genre_url:
                    await check_genre_complete_and_notify(genre_name, genre_url)
                genre_complete_checked.add(genre_name)

async def loop_once_multi_sites(force_unskip=False):
    now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"\n===== [START] Check missing for all sites at {now} =====")
    tasks = []
    for site_key, url in BASE_URLS.items():
        adapter = get_adapter(site_key)
        tasks.append(check_and_crawl_missing_all_stories(adapter, url, site_key=site_key, force_unskip=force_unskip))
    try:
        await asyncio.gather(*tasks)
    except Exception as e:
        print(f"[ERROR] Lá»—i khi kiá»ƒm tra/crawl missing: {e}")
    print(f"===== [DONE] =====\n")


if __name__ == "__main__":
    args = parse_args()
    asyncio.run(loop_once_multi_sites(force_unskip=args.force_unskip))
